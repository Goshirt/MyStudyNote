filebeat.inputs:
# -----
# eos-trace-log
# -----
- type: log
  paths: ["/opt/eos8/demo/trace.log*"]
  encoding: UTF-8
  fields:
    eos-log-type: eos-trace-log
  exclude_files: [".gz$"]
  multiline:
    pattern: '^\d{4}\-\d{2}\-\d{2}'
    negate: true
    match: after
    max_lines: 500
# -----
# eos-dap-trace-log
# -----
- type: docker
  combine_partial: true
  containers:
    path: "/usr/share/dockerlogs/data"
    stream: "stdout"
    ids:
      - "*"
  exclude_files: ['\.gz$']
  ignore_older: 10m


processors:
  # decode the log field (sub JSON document) if JSON encoded, then maps it's fields to elasticsearch fields
- decode_json_fields:
    fields: ["log", "message"]
    target: ""
    # overwrite existing target elasticsearch fields while decoding json fields
    overwrite_keys: true
- add_docker_metadata:
    host: "unix:///var/run/docker.sock"

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml

# -----
# output
# -----
output.kafka:
  enabled: true
  hosts: ["kafka:9092"]
  topic: '%{[fields.eos-log-type]}'
  worker: 2
  max_retries: 3
  bulk_max_size: 2048
  timeout: 30s
  broker_timeout: 10s
  channel_buffer_size: 256
  keep_alive: 60
  compression: gzip
  max_message_bytes: 1000000
  required_acks: 1
  client_id: 'eos-filebeats'  
  reload.enabled: false  